{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/mlenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from models.modules.multihead import *\n",
    "from utils.prepare_data import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(context=\"talk\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math, copy, time\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "EMBEDDING_SIZE = 10\n",
    "HIDDEN_SIZE = 512\n",
    "ATTENTION_SIZE = 64\n",
    "lr = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "KEEP_PROB = 0.5\n",
    "LAMBDA = 0.0001\n",
    "\n",
    "VOCAB_SIZE = 5\n",
    "\n",
    "MAX_LABEL = 2\n",
    "\n",
    "GENOME_LENGTH = 20000\n",
    "CONTEXT_SIZE = GENOME_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDenseEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramDenseEmbedding, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLR(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, d_model, h, dropout=0.1):\n",
    "        super(AttentionLR, self).__init__()\n",
    "        \n",
    "        assert d_model % h == 0\n",
    "        \n",
    "        # We assume d_v always equals d_k as defined in the paper\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        \n",
    "        self.h = h\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.KQ_attn = None\n",
    "        self.KQV_attn = None\n",
    "        self.linears = clones(nn.Linear(input_size, num_classes),1)\n",
    "        \n",
    "        self.K = NGramDenseEmbedding(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)\n",
    "        self.V = NGramDenseEmbedding(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, query_seq):\n",
    "        \n",
    "        Q_lookup = torch.tensor([word_to_ix[w] for w in query_seq], dtype=torch.long)\n",
    "        \n",
    "        K_lookup = self.K(Q_lookup)\n",
    "        V_lookup = self.V(Q_lookup)       \n",
    "        \n",
    "        self.KQ_attn, self.KQV_attn = attention(Q_lookup, K_lookup, V_lookup, dropout=self.dropout)\n",
    "                \n",
    "        return F.log_softmax(self.linear(self.KQV_attn), dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 2)\n",
      "(155244, 2)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "x_train, y_train = load_data(\"../data/train-BRAF.csv\", sample_ratio=1)\n",
    "x_test, y_test = load_data(\"../data/test-BRAF.csv\", sample_ratio=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 100)\n",
      "(155244, 100)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {'N':0, 'A':1, 'C':2, 'T':3, 'G':4}\n",
    "x_train_l = np.ndarray((len(x_train),MAX_SEQ_LENGTH))\n",
    "\n",
    "for t in np.arange(len(x_train)):\n",
    "    line = list(x_train[t])[1:MAX_SEQ_LENGTH+1]\n",
    "    for k in np.arange(MAX_SEQ_LENGTH):\n",
    "        x_train_l[t,k] = word_to_ix[line[k]]\n",
    "    \n",
    "x_test_l = np.ndarray((len(x_test),MAX_SEQ_LENGTH))\n",
    "\n",
    "for t in np.arange(len(x_test)):    \n",
    "    line = list(x_test[t])[1:MAX_SEQ_LENGTH+1]\n",
    "    for k in np.arange(MAX_SEQ_LENGTH):\n",
    "        x_test_l[t,k] = word_to_ix[line[k]]\n",
    "    \n",
    "print(x_train_l.shape)\n",
    "print(x_test_l.shape)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 100)\n",
      "(155244, 100)\n",
      "[3. 3. 1. 4. 3. 3. 1. 1. 1. 1. 1. 3. 4. 1. 4. 4. 3. 3. 4. 4. 1. 1. 1. 4.\n",
      " 3. 1. 1. 1. 3. 2. 3. 4. 1. 2. 2. 3. 1. 4. 1. 3. 4. 1. 3. 3. 4. 4. 3. 3.\n",
      " 3. 4. 1. 2. 1. 1. 3. 4. 1. 4. 4. 1. 3. 3. 1. 1. 2. 3. 3. 3. 1. 2. 3. 1.\n",
      " 1. 3. 3. 1. 4. 1. 3. 3. 1. 3. 4. 3. 4. 4. 2. 3. 4. 1. 3. 4. 4. 4. 3. 4.\n",
      " 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_l.shape)\n",
    "print(x_test_l.shape)\n",
    "\n",
    "print(x_train_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n",
      "(1000, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train_l[0:10000,:]\n",
    "x_test = x_test_l[0:1000,:]\n",
    "\n",
    "y_train = y_train[0:10000,:]\n",
    "y_test = y_test[0:1000,:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data preprocessing\n",
    "# x_train, x_test, vocab, VOCAB_SIZE = \\\n",
    "#     data_preprocessing(x_train, x_test, MAX_SEQ_LENGTH)\n",
    "# print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100\n",
      "Validation size:  100\n"
     ]
    }
   ],
   "source": [
    "# split dataset to test and dev\n",
    "x_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n",
    "    split_dataset(x_test, y_test, 0.1)\n",
    "print(\"Validation size: \", dev_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv ret: (?, 100, 128)\n",
      "(?, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    batch_x = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH])\n",
    "    batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBEDDING_SIZE], -1.0, 1.0), trainable=True)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_x)\n",
    "    # multihead attention\n",
    "    outputs = multihead_attention(queries=batch_embedded, keys=batch_embedded)\n",
    "    # FFN(x) = LN(x + point-wisely NN(x))\n",
    "    outputs = feedforward(outputs, [HIDDEN_SIZE, EMBEDDING_SIZE])\n",
    "    print(outputs.shape)\n",
    "    outputs = tf.reshape(outputs, [-1, MAX_SEQ_LENGTH * EMBEDDING_SIZE])\n",
    "    logits = tf.layers.dense(outputs, units=MAX_LABEL)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=batch_y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    prediction = tf.argmax(tf.nn.softmax(logits), 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(batch_y, 1)), tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized! \n",
      "Start trainning\n",
      "Epoch 1 start !\n",
      "Validation accuracy and loss:  [0.47, 0.80978376]\n",
      "epoch time: 172.9900119304657  s\n",
      "Epoch 2 start !\n",
      "Validation accuracy and loss:  [0.46, 0.72138095]\n",
      "epoch time: 187.37515902519226  s\n",
      "Epoch 3 start !\n",
      "Validation accuracy and loss:  [0.5, 0.76346606]\n",
      "epoch time: 152.2712278366089  s\n",
      "Epoch 4 start !\n",
      "Validation accuracy and loss:  [0.6, 0.69934523]\n",
      "epoch time: 439.39326095581055  s\n",
      "Epoch 5 start !\n",
      "Validation accuracy and loss:  [0.38, 0.80945414]\n",
      "epoch time: 121.4345600605011  s\n",
      "Training finished, time consumed :  1077.1485340595245  s\n",
      "start predicting:  \n",
      "\n",
      "Test accuracy : 42.444444 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 5\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized! \")\n",
    "\n",
    "    print(\"Start trainning\")\n",
    "    start = time.time()\n",
    "    for e in range(epochs):\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch %d start !\" % (e + 1))\n",
    "        for x_batch, y_batch in fill_feed_dict(x_train, y_train, BATCH_SIZE):\n",
    "            fd = {batch_x: x_batch, batch_y: y_batch, keep_prob: KEEP_PROB}\n",
    "            l, _, acc = sess.run([loss, optimizer, accuracy], feed_dict=fd)\n",
    "\n",
    "        epoch_finish = time.time()\n",
    "        print(\"Validation accuracy and loss: \", sess.run([accuracy, loss], feed_dict={\n",
    "            batch_x: x_dev,\n",
    "            batch_y: y_dev,\n",
    "            keep_prob: 1.0\n",
    "        }))\n",
    "        print(\"epoch time:\", epoch_finish - epoch_start , \" s\")\n",
    "\n",
    "    print(\"Training finished, time consumed : \", time.time() - start, \" s\")\n",
    "    print(\"start predicting:  \\n\")\n",
    "    test_accuracy = sess.run([accuracy], feed_dict={batch_x: x_test, batch_y: y_test, keep_prob: 1})\n",
    "    print(\"Test accuracy : %f %%\" % (test_accuracy[0] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
