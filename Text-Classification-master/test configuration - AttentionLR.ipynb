{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from models.modules.multihead import *\n",
    "from utils.prepare_data import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(context=\"talk\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "\n",
    "import math, copy, time\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "EMBEDDING_SIZE = 10\n",
    "HIDDEN_SIZE = 512\n",
    "ATTENTION_SIZE = 64\n",
    "\n",
    "lr = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "KEEP_PROB = 0.5\n",
    "LAMBDA = 0.0001\n",
    "\n",
    "VOCAB_SIZE = 5\n",
    "\n",
    "MAX_LABEL = 2\n",
    "\n",
    "GENOME_LENGTH = 20000\n",
    "CONTEXT_SIZE = GENOME_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedReadDataset(data.Dataset):\n",
    "    'Characterizes a Tensorized dataset for genome reads in PyTorch'\n",
    "    \n",
    "    def __init__(self, reads, ref_locs, labels, read_length=100, genome_start=0, genome_end):\n",
    "        'Initialization'\n",
    "        \n",
    "        self.read_length = read_length\n",
    "        self.labels = labels\n",
    "        self.reads = reads\n",
    "        self.ref_locs = ref_locs\n",
    "        self.genome_start = genome_start\n",
    "        self.genome_end = genome_end\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.reads)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        \n",
    "        vals = list(self.reads[index])[1:self.read_length+1]\n",
    "        \n",
    "        locs = list(np.arange(self.ref_locs-self.genome_start,self.ref_locs+100-self.genome_start))\n",
    "\n",
    "        vals2idx = {'N': 0, 'A': 1, 'C': 2, 'T': 3, 'G': 4}\n",
    "        read = torch.LongTensor(np.array([vals2idx[val]+loc*len(vals2idx) for val, loc in zip(vals, locs)], dtype=int), requires_grad=False)\n",
    "\n",
    "        X = read\n",
    "        Y = self.labels[index,:]\n",
    "\n",
    "        return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, dropout=None):\n",
    "    \n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDenseEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramDenseEmbedding, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLR(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, dropout=0.1):\n",
    "        super(AttentionLR, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.KQ_attn = None\n",
    "        self.KQV_attn = None\n",
    "        self.linears = clones(nn.Linear(input_size, num_classes),1)\n",
    "        \n",
    "        self.K = NGramDenseEmbedding(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)\n",
    "        self.V = NGramDenseEmbedding(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, query_seq):\n",
    "        \n",
    "        Q_lookup = torch.tensor([word_to_ix[w] for w in query_seq], dtype=torch.long)\n",
    "        \n",
    "        K_lookup = self.K(Q_lookup)\n",
    "        V_lookup = self.V(Q_lookup)       \n",
    "        \n",
    "        self.KQ_attn, self.KQV_attn = attention(Q_lookup, K_lookup, V_lookup, dropout=self.dropout)\n",
    "                \n",
    "        return F.log_softmax(self.linear(self.KQV_attn), dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceAttentionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, genome_length, read_length=100, vocab_size=5, query_size=64, embedding_size=128, num_classes=2):\n",
    "        super(SequenceAttentionClassifier, self).__init__()\n",
    "        self.genome_length = genome_length\n",
    "        self.read_length = read_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.query_size = query_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_classes = num_classes\n",
    "        self.K = nn.Embedding(vocab_size*genome_length, embedding_size)\n",
    "        self.V = nn.Embedding(vocab_size*genome_length, query_size)\n",
    "        self.W = nn.Linear(query_size, num_classes)\n",
    "        self.Q = nn.Linear(embedding_size, query_size)\n",
    "        \n",
    "    def forward(self, read):\n",
    "        \n",
    "        # 'read' here should be mapped to a flattened form where X_ij = 1 maps to i*vocab_size + j\n",
    "        K_lookup = self.K(read) # Get the relevant keys\n",
    "        V_lookup = self.V(read) # Get the relevant values\n",
    "\n",
    "        # Get the attention weights\n",
    "        logits = self.Q(K_lookup) / math.sqrt(self.embedding_size)\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        \n",
    "        # Calculate the covariates for the logistic regression\n",
    "        X = torch.matmul(probs, V_lookup)\n",
    "\n",
    "        # Right now we can just ignore the fact that we're doing a linear-transform.\n",
    "        # In the future we'll add nonlinearities\n",
    "\n",
    "        # Return the logits for the classifier\n",
    "        return self.W(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 2)\n",
      "(155244, 2)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "x_train, refs_train, y_train = load_ref_data(\"../data/train-BRAF.csv\", sample_ratio=1)\n",
    "x_test, refs_test, y_test = load_ref_data(\"../data/test-BRAF.csv\", sample_ratio=1)\n",
    "\n",
    "# split dataset to test and dev\n",
    "x_train, x_softval, y_train, y_softval, ref_train, ref_softval, softval_size, train_size = \\\n",
    "    split_ref_dataset(x_train, y_train, ref_train, 0.01)\n",
    "    \n",
    "print(\"Soft Validation size: \", dev_size)\n",
    "print(\"Training size: \", train_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 100)\n",
      "(155244, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_l = np.ndarray((len(x_train),MAX_SEQ_LENGTH))\n",
    "\n",
    "for t in np.arange(len(x_train)):\n",
    "    line = list(x_train[t])[1:MAX_SEQ_LENGTH+1]\n",
    "    for k in np.arange(MAX_SEQ_LENGTH):\n",
    "        x_train_l[t,k] = word_to_ix[line[k]]\n",
    "    \n",
    "x_test_l = np.ndarray((len(x_test),MAX_SEQ_LENGTH))\n",
    "\n",
    "for t in np.arange(len(x_test)):    \n",
    "    line = list(x_test[t])[1:MAX_SEQ_LENGTH+1]\n",
    "    for k in np.arange(MAX_SEQ_LENGTH):\n",
    "        x_test_l[t,k] = word_to_ix[line[k]]\n",
    "    \n",
    "print(x_train_l.shape)\n",
    "print(x_test_l.shape)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 100)\n",
      "(155244, 100)\n",
      "[3. 3. 1. 4. 3. 3. 1. 1. 1. 1. 1. 3. 4. 1. 4. 4. 3. 3. 4. 4. 1. 1. 1. 4.\n",
      " 3. 1. 1. 1. 3. 2. 3. 4. 1. 2. 2. 3. 1. 4. 1. 3. 4. 1. 3. 3. 4. 4. 3. 3.\n",
      " 3. 4. 1. 2. 1. 1. 3. 4. 1. 4. 4. 1. 3. 3. 1. 1. 2. 3. 3. 3. 1. 2. 3. 1.\n",
      " 1. 3. 3. 1. 4. 1. 3. 3. 1. 3. 4. 3. 4. 4. 2. 3. 4. 1. 3. 4. 4. 4. 3. 4.\n",
      " 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_l.shape)\n",
    "print(x_test_l.shape)\n",
    "\n",
    "print(x_train_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n",
      "(1000, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train_l[0:10000,:]\n",
    "x_test = x_test_l[0:1000,:]\n",
    "\n",
    "y_train = y_train[0:10000,:]\n",
    "y_test = y_test[0:1000,:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pipeline\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data preprocessing\n",
    "# x_train, x_test, vocab, VOCAB_SIZE = \\\n",
    "#     data_preprocessing(x_train, x_test, MAX_SEQ_LENGTH)\n",
    "# print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100\n",
      "Validation size:  100\n"
     ]
    }
   ],
   "source": [
    "# split dataset to test and dev\n",
    "\n",
    "x_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n",
    "    split_dataset(x_test, y_test, 0.1)\n",
    "print(\"Validation size: \", dev_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv ret: (?, 100, 128)\n",
      "(?, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(partition['train'], labels), \n",
    "validation_set_1 = Dataset(partition['validation'], labels)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model = AttentionLR(MAX_SEQ_LENGTH, MAX_LABEL = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Training process\n",
    "for epoch in range(num_epochs):\n",
    "    for local_batch, local_labels in training_generator:\n",
    "\n",
    "\n",
    "# Validation process\n",
    "for local_batch, local_labels in validation_generator:\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        seq = Variable(images.view(-1, 28 * 28))  # Images flattened into 1D tensors\n",
    "        labels = Variable(labels)  # Labels \n",
    "        \n",
    "        # Forward -> Backprop -> Optimize\n",
    "        optimizer.zero_grad()  # Manually zero the gradient buffers\n",
    "        outputs = model(images)  # Predict the class using the test set\n",
    "                \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Epoch {}, loss :{}\".format(epoch + 1, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
