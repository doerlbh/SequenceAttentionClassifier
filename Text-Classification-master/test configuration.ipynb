{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/mlenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from models.modules.multihead import *\n",
    "from utils.prepare_data import *\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter\n",
    "MAX_SEQ_LENGTH = 100\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 512\n",
    "ATTENTION_SIZE = 64\n",
    "lr = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "KEEP_PROB = 0.5\n",
    "LAMBDA = 0.0001\n",
    "\n",
    "vocab_size = 5\n",
    "\n",
    "MAX_LABEL = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 2)\n",
      "(155244, 2)\n"
     ]
    }
   ],
   "source": [
    "# x_train_0 = pd.read_csv('../data/NACTG-train-normal-BRAF.txt', sep=',',header=None)\n",
    "# x_train_1 = pd.read_csv('../data/NACTG-train-tumor-BRAF.txt', sep=',',header=None)\n",
    "# x_test_0 = pd.read_csv('../data/NACTG-test-normal-BRAF.txt', sep=',',header=None)\n",
    "# x_test_1 = pd.read_csv('../data/NACTG-test-tumor-BRAF.txt', sep=',',header=None)\n",
    "\n",
    "# x_train_0.head()\n",
    "\n",
    "# load data\n",
    "x_train, y_train = load_data(\"../data/train-BRAF.csv\", sample_ratio=1)\n",
    "x_test, y_test = load_data(\"../data/test-BRAF.csv\", sample_ratio=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 100)\n",
      "(155244, 100)\n"
     ]
    }
   ],
   "source": [
    "base_number = {'N':0, 'A':1, 'C':2, 'T':3, 'G':4}\n",
    "x_train_l = np.ndarray((len(x_train),MAX_SEQ_LENGTH))\n",
    "\n",
    "for t in np.arange(len(x_train)):\n",
    "    line = list(x_train[t])[1:MAX_SEQ_LENGTH+1]\n",
    "    for k in np.arange(MAX_SEQ_LENGTH):\n",
    "        x_train_l[t,k] = base_number[line[k]]\n",
    "    \n",
    "x_test_l = np.ndarray((len(x_test),MAX_SEQ_LENGTH))\n",
    "\n",
    "for t in np.arange(len(x_test)):    \n",
    "    line = list(x_test[t])[1:MAX_SEQ_LENGTH+1]\n",
    "    for k in np.arange(MAX_SEQ_LENGTH):\n",
    "        x_test_l[t,k] = base_number[line[k]]\n",
    "    \n",
    "print(x_train_l.shape)\n",
    "print(x_test_l.shape)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 100)\n",
      "(155244, 100)\n",
      "[3. 3. 1. 4. 3. 3. 1. 1. 1. 1. 1. 3. 4. 1. 4. 4. 3. 3. 4. 4. 1. 1. 1. 4.\n",
      " 3. 1. 1. 1. 3. 2. 3. 4. 1. 2. 2. 3. 1. 4. 1. 3. 4. 1. 3. 3. 4. 4. 3. 3.\n",
      " 3. 4. 1. 2. 1. 1. 3. 4. 1. 4. 4. 1. 3. 3. 1. 1. 2. 3. 3. 3. 1. 2. 3. 1.\n",
      " 1. 3. 3. 1. 4. 1. 3. 3. 1. 3. 4. 3. 4. 4. 2. 3. 4. 1. 3. 4. 4. 4. 3. 4.\n",
      " 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_l.shape)\n",
    "print(x_test_l.shape)\n",
    "\n",
    "print(x_train_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_l[0:10000,:]\n",
    "x_test = x_test_l[0:1000,:]\n",
    "\n",
    "y_train = y_train[0:10000,:]\n",
    "y_test = y_test[0:1000,:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data preprocessing\n",
    "# x_train, x_test, vocab, vocab_size = \\\n",
    "#     data_preprocessing(x_train, x_test, MAX_SEQ_LENGTH)\n",
    "# print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100\n",
      "Validation size:  100\n"
     ]
    }
   ],
   "source": [
    "# split dataset to test and dev\n",
    "x_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n",
    "    split_dataset(x_test, y_test, 0.1)\n",
    "print(\"Validation size: \", dev_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv ret: (?, 100, 128)\n",
      "(?, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    batch_x = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH])\n",
    "    batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([vocab_size, EMBEDDING_SIZE], -1.0, 1.0), trainable=True)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_x)\n",
    "    # multihead attention\n",
    "    outputs = multihead_attention(queries=batch_embedded, keys=batch_embedded)\n",
    "    # FFN(x) = LN(x + point-wisely NN(x))\n",
    "    outputs = feedforward(outputs, [HIDDEN_SIZE, EMBEDDING_SIZE])\n",
    "    print(outputs.shape)\n",
    "    outputs = tf.reshape(outputs, [-1, MAX_SEQ_LENGTH * EMBEDDING_SIZE])\n",
    "    logits = tf.layers.dense(outputs, units=MAX_LABEL)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=batch_y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    prediction = tf.argmax(tf.nn.softmax(logits), 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(batch_y, 1)), tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized! \n",
      "Start trainning\n",
      "Epoch 1 start !\n",
      "Validation accuracy and loss:  [0.47, 0.80978376]\n",
      "epoch time: 172.9900119304657  s\n",
      "Epoch 2 start !\n",
      "Validation accuracy and loss:  [0.46, 0.72138095]\n",
      "epoch time: 187.37515902519226  s\n",
      "Epoch 3 start !\n",
      "Validation accuracy and loss:  [0.5, 0.76346606]\n",
      "epoch time: 152.2712278366089  s\n",
      "Epoch 4 start !\n",
      "Validation accuracy and loss:  [0.6, 0.69934523]\n",
      "epoch time: 439.39326095581055  s\n",
      "Epoch 5 start !\n",
      "Validation accuracy and loss:  [0.38, 0.80945414]\n",
      "epoch time: 121.4345600605011  s\n",
      "Training finished, time consumed :  1077.1485340595245  s\n",
      "start predicting:  \n",
      "\n",
      "Test accuracy : 42.444444 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 5\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized! \")\n",
    "\n",
    "    print(\"Start trainning\")\n",
    "    start = time.time()\n",
    "    for e in range(epochs):\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch %d start !\" % (e + 1))\n",
    "        for x_batch, y_batch in fill_feed_dict(x_train, y_train, BATCH_SIZE):\n",
    "            fd = {batch_x: x_batch, batch_y: y_batch, keep_prob: KEEP_PROB}\n",
    "            l, _, acc = sess.run([loss, optimizer, accuracy], feed_dict=fd)\n",
    "\n",
    "        epoch_finish = time.time()\n",
    "        print(\"Validation accuracy and loss: \", sess.run([accuracy, loss], feed_dict={\n",
    "            batch_x: x_dev,\n",
    "            batch_y: y_dev,\n",
    "            keep_prob: 1.0\n",
    "        }))\n",
    "        print(\"epoch time:\", epoch_finish - epoch_start , \" s\")\n",
    "\n",
    "    print(\"Training finished, time consumed : \", time.time() - start, \" s\")\n",
    "    print(\"start predicting:  \\n\")\n",
    "    test_accuracy = sess.run([accuracy], feed_dict={batch_x: x_test, batch_y: y_test, keep_prob: 1})\n",
    "    print(\"Test accuracy : %f %%\" % (test_accuracy[0] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
