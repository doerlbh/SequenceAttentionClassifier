{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/mlenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from models.modules.multihead import *\n",
    "from utils.prepare_data import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(context=\"talk\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math, copy, time\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "EMBEDDING_SIZE = 10\n",
    "HIDDEN_SIZE = 512\n",
    "ATTENTION_SIZE = 64\n",
    "\n",
    "lr = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "KEEP_PROB = 0.5\n",
    "LAMBDA = 0.0001\n",
    "\n",
    "VOCAB_SIZE = 5\n",
    "\n",
    "MAX_LABEL = 2\n",
    "\n",
    "GENOME_LENGTH = 20000\n",
    "CONTEXT_SIZE = GENOME_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, dropout=None):\n",
    "    \n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDenseEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramDenseEmbedding, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLR(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, dropout=0.1):\n",
    "        super(AttentionLR, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.KQ_attn = None\n",
    "        self.KQV_attn = None\n",
    "        self.linears = clones(nn.Linear(input_size, num_classes),1)\n",
    "        \n",
    "        self.K = NGramDenseEmbedding(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)\n",
    "        self.V = NGramDenseEmbedding(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, query_seq):\n",
    "        \n",
    "        Q_lookup = torch.tensor([word_to_ix[w] for w in query_seq], dtype=torch.long)\n",
    "        \n",
    "        K_lookup = self.K(Q_lookup)\n",
    "        V_lookup = self.V(Q_lookup)       \n",
    "        \n",
    "        self.KQ_attn, self.KQV_attn = attention(Q_lookup, K_lookup, V_lookup, dropout=self.dropout)\n",
    "                \n",
    "        return F.log_softmax(self.linear(self.KQV_attn), dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceAttentionClassifier(nn.Module):\n",
    "    def __init__(self, genome_length, read_length, vocab_size=5, query_size=64, embedding_size=128, num_classes=2):\n",
    "        super(SequenceAttentionClassifier, self).__init__()\n",
    "        self.genome_length = genome_length\n",
    "        self.read_length = read_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.query_size = query_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_classes = num_classes\n",
    "        self.K = nn.Embedding(vocab_size*genome_length, embedding_size)\n",
    "        self.V = nn.Embedding(vocab_size*genome_length, query_size)\n",
    "        self.W = nn.Linear(query_size, num_classes)\n",
    "        self.Q = nn.Linear(embedding_size, query_size)\n",
    "        \n",
    "    def forward(self, read):\n",
    "        # 'read' here should be mapped to a flattened form where X_ij = 1 maps to i*vocab_size + j\n",
    "        K_lookup = self.K(read) # Get the relevant keys\n",
    "        V_lookup = self.V(read) # Get the relevant values\n",
    "\n",
    "        # Get the attention weights\n",
    "        logits = self.Q(K_lookup) / math.sqrt(self.embedding_size)\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        \n",
    "        # Calculate the covariates for the logistic regression\n",
    "        X = torch.matmul(probs, V_lookup)\n",
    "\n",
    "        # Right now we can just ignore the fact that we're doing a linear-transform.\n",
    "        # In the future we'll add nonlinearities\n",
    "\n",
    "        # Return the logits for the classifier\n",
    "        return self.W(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 2)\n",
      "(155244, 2)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "x_train, y_train = load_data(\"../data/train-BRAF.csv\", sample_ratio=1)\n",
    "x_test, y_test = load_data(\"../data/test-BRAF.csv\", sample_ratio=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 100)\n",
      "(155244, 100)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {'N':0, 'A':1, 'C':2, 'T':3, 'G':4}\n",
    "x_train_l = np.ndarray((len(x_train),MAX_SEQ_LENGTH))\n",
    "\n",
    "for t in np.arange(len(x_train)):\n",
    "    line = list(x_train[t])[1:MAX_SEQ_LENGTH+1]\n",
    "    for k in np.arange(MAX_SEQ_LENGTH):\n",
    "        x_train_l[t,k] = word_to_ix[line[k]]\n",
    "    \n",
    "x_test_l = np.ndarray((len(x_test),MAX_SEQ_LENGTH))\n",
    "\n",
    "for t in np.arange(len(x_test)):    \n",
    "    line = list(x_test[t])[1:MAX_SEQ_LENGTH+1]\n",
    "    for k in np.arange(MAX_SEQ_LENGTH):\n",
    "        x_test_l[t,k] = word_to_ix[line[k]]\n",
    "    \n",
    "print(x_train_l.shape)\n",
    "print(x_test_l.shape)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440198, 100)\n",
      "(155244, 100)\n",
      "[3. 3. 1. 4. 3. 3. 1. 1. 1. 1. 1. 3. 4. 1. 4. 4. 3. 3. 4. 4. 1. 1. 1. 4.\n",
      " 3. 1. 1. 1. 3. 2. 3. 4. 1. 2. 2. 3. 1. 4. 1. 3. 4. 1. 3. 3. 4. 4. 3. 3.\n",
      " 3. 4. 1. 2. 1. 1. 3. 4. 1. 4. 4. 1. 3. 3. 1. 1. 2. 3. 3. 3. 1. 2. 3. 1.\n",
      " 1. 3. 3. 1. 4. 1. 3. 3. 1. 3. 4. 3. 4. 4. 2. 3. 4. 1. 3. 4. 4. 4. 3. 4.\n",
      " 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_l.shape)\n",
    "print(x_test_l.shape)\n",
    "\n",
    "print(x_train_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n",
      "(1000, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train_l[0:10000,:]\n",
    "x_test = x_test_l[0:1000,:]\n",
    "\n",
    "y_train = y_train[0:10000,:]\n",
    "y_test = y_test[0:1000,:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data preprocessing\n",
    "# x_train, x_test, vocab, VOCAB_SIZE = \\\n",
    "#     data_preprocessing(x_train, x_test, MAX_SEQ_LENGTH)\n",
    "# print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100\n",
      "Validation size:  100\n"
     ]
    }
   ],
   "source": [
    "# split dataset to test and dev\n",
    "x_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n",
    "    split_dataset(x_test, y_test, 0.1)\n",
    "print(\"Validation size: \", dev_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv ret: (?, 100, 128)\n",
      "(?, 100, 128)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model = AttentionLR(MAX_SEQ_LENGTH, MAX_LABEL = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        seq = Variable(images.view(-1, 28 * 28))  # Images flattened into 1D tensors\n",
    "        labels = Variable(labels)  # Labels \n",
    "        \n",
    "        # Forward -> Backprop -> Optimize\n",
    "        optimizer.zero_grad()  # Manually zero the gradient buffers\n",
    "        outputs = model(images)  # Predict the class using the test set\n",
    "                \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Epoch {}, loss :{}\".format(epoch + 1, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
