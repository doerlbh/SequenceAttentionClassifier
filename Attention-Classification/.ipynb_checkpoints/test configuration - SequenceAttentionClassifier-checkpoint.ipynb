{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(context=\"talk\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "\n",
    "QUERY_SIZE = 64\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "# HIDDEN_SIZE = 512\n",
    "# ATTENTION_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "VOCAB_SIZE = 5\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Data-specific\n",
    "\n",
    "READ_LENGTH = 100\n",
    "\n",
    "GENOME_START = 140719327\n",
    "GENOME_END = 140924764\n",
    "\n",
    "GENOME_LENGTH = GENOME_END - GENOME_START + 1\n",
    "CONTEXT_SIZE = GENOME_LENGTH\n",
    "\n",
    "ref_names = [\"class\", \"ref\", \"sequence\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ref_data(file_name, sample_ratio= 1, n_class=2, names=ref_names):\n",
    "    \n",
    "    csv_file = pd.read_csv(file_name, names=ref_names)\n",
    "    shuffle_csv = csv_file.sample(frac=sample_ratio).reset_index()\n",
    "#     x = pd.Series(shuffle_csv[\"sequence\"])\n",
    "    x = list(shuffle_csv[\"sequence\"])\n",
    "#     ref = pd.Series(shuffle_csv[\"ref\"])\n",
    "    ref = list(shuffle_csv[\"ref\"])\n",
    "    y = pd.Series(shuffle_csv[\"class\"])\n",
    "    y = to_one_hot(y, n_class)\n",
    "    print(y.shape)\n",
    "#     print(type(x))\n",
    "#     print(type(y))\n",
    "#     print(type(ref))\n",
    "\n",
    "    return x, ref, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_class):\n",
    "    \n",
    "    return np.eye(n_class)[y.astype(int)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ref_dataset(x_test, y_test, ref_test, dev_ratio):\n",
    "\n",
    "    test_size = len(x_test)\n",
    "    print(test_size)\n",
    "    dev_size = (int)(test_size * dev_ratio)\n",
    "    print(dev_size)\n",
    "\n",
    "    x_dev = x_test[:dev_size]\n",
    "    x_test = x_test[dev_size:]\n",
    "    y_dev = y_test[:dev_size]\n",
    "    y_test = y_test[dev_size:]\n",
    "    ref_dev = ref_test[:dev_size]\n",
    "    ref_test = ref_test[dev_size:]\n",
    "\n",
    "    return x_test, x_dev, y_test, y_dev, ref_test, ref_dev, dev_size, test_size - dev_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedReadDataset(torch.utils.data.DataLoader):\n",
    "    'Characterizes a Tensorized dataset for genome reads in PyTorch'\n",
    "    \n",
    "    def __init__(self, reads, ref_locs, labels, read_length=100, genome_start=0, genome_end=0):\n",
    "#         super(TensorizedReadDataset, self).__init__()\n",
    "        \n",
    "        self.read_length = read_length\n",
    "        self.labels = labels\n",
    "        self.reads = reads\n",
    "        self.ref_locs = ref_locs\n",
    "        self.genome_start = genome_start\n",
    "        self.genome_end = genome_end\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reads)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        vals = list(self.reads[index])[0:self.read_length]        \n",
    "        locs = list(np.arange(self.ref_locs[index]-self.genome_start,self.ref_locs[index]+self.read_length-self.genome_start))\n",
    "\n",
    "#         print(len(vals))\n",
    "#         print(len(locs))\n",
    "        \n",
    "        vals2idx = {'N': 0, 'A': 1, 'C': 2, 'T': 3, 'G': 4}\n",
    "#         read = torch.LongTensor(np.array([vals2idx[val]+loc*len(vals2idx) for val, loc in zip(vals, locs)], dtype=int), requires_grad=False)\n",
    "\n",
    "        read = torch.autograd.Variable(torch.LongTensor(np.array([vals2idx[val]+loc*len(vals2idx) for val, loc in zip(vals, locs)], dtype=int)), requires_grad=False)\n",
    "        \n",
    "        X = read\n",
    "        Y = self.labels[index,:]\n",
    "\n",
    "#         torch.LongTensor(self.labels[index,:])\n",
    "\n",
    "        return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceAttentionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, genome_length, read_length=100, vocab_size=5, query_size=64, embedding_size=128, num_classes=2):\n",
    "        super(SequenceAttentionClassifier, self).__init__()\n",
    "        self.genome_length = genome_length\n",
    "        self.read_length = read_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.query_size = query_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_classes = num_classes\n",
    "        self.K = nn.Embedding(vocab_size*genome_length, embedding_size)\n",
    "        self.V = nn.Embedding(vocab_size*genome_length, query_size)\n",
    "        self.W = nn.Linear(query_size, num_classes)\n",
    "        self.Q = nn.Linear(embedding_size, query_size)\n",
    "        \n",
    "    def forward(self, read):\n",
    "        \n",
    "        # 'read' here should be mapped to a flattened form where X_ij = 1 maps to i*vocab_size + j\n",
    "        K_lookup = self.K(read) # Get the relevant keys\n",
    "        V_lookup = self.V(read) # Get the relevant values\n",
    "\n",
    "        # Get the attention weights\n",
    "        logits = self.Q(K_lookup) / math.sqrt(self.embedding_size)\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "                \n",
    "        # Calculate the covariates for the logistic regression\n",
    "#         X = torch.matmul(probs.transpose(1,2), V_lookup)\n",
    "#         X = probs * V_lookup\n",
    "        X = (probs * V_lookup).sum(dim=1)\n",
    "\n",
    "        # Right now we can just ignore the fact that we're doing a linear-transform.\n",
    "        # In the future we'll add nonlinearities\n",
    "\n",
    "        # Return the logits for the classifier\n",
    "        return self.W(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2439452, 2)\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "(155205, 2)\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "2439452\n",
      "24394\n",
      "Soft Validation size:  24394\n",
      "Training size:  2415058\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "x_train, refs_train, y_train = load_ref_data(\"../data/ref-train-BRAF.csv\", sample_ratio=1)\n",
    "x_test, refs_test, y_test = load_ref_data(\"../data/ref-test-BRAF.csv\", sample_ratio=1)\n",
    "\n",
    "# split dataset to test and dev\n",
    "x_train, x_softval, y_train, y_softval, refs_train, refs_softval, softval_size, train_size = \\\n",
    "    split_ref_dataset(x_train, y_train, refs_train, 0.01)\n",
    "    \n",
    "print(\"Soft Validation size: \", softval_size)\n",
    "print(\"Training size: \", train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "train_dataset = TensorizedReadDataset(reads=x_train, \n",
    "                                      ref_locs=refs_train, \n",
    "                                      labels=y_train, \n",
    "                                      read_length=READ_LENGTH, \n",
    "                                      genome_start=GENOME_START, \n",
    "                                      genome_end=GENOME_END)\n",
    "\n",
    "hardval_dataset = TensorizedReadDataset(reads=x_test, \n",
    "                                        ref_locs=refs_test, \n",
    "                                        labels=y_test, \n",
    "                                        read_length=READ_LENGTH, \n",
    "                                        genome_start=GENOME_START, \n",
    "                                        genome_end=GENOME_END)\n",
    "\n",
    "softval_dataset = TensorizedReadDataset(reads=x_softval, \n",
    "                                        ref_locs=refs_softval, \n",
    "                                        labels=y_softval, \n",
    "                                        read_length=READ_LENGTH, \n",
    "                                        genome_start=GENOME_START, \n",
    "                                        genome_end=GENOME_END)\n",
    "\n",
    "# Input pipeline\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "hardval_loader = torch.utils.data.DataLoader(dataset=hardval_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True)\n",
    "\n",
    "softval_loader = torch.utils.data.DataLoader(dataset=softval_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AttentionLR(MAX_SEQ_LENGTH, MAX_LABEL)\n",
    "model = SequenceAttentionClassifier(genome_length=GENOME_LENGTH,\n",
    "                                    read_length=READ_LENGTH,\n",
    "                                    vocab_size=VOCAB_SIZE,\n",
    "                                    query_size=QUERY_SIZE,\n",
    "                                    embedding_size=EMBEDDING_SIZE,\n",
    "                                    num_classes=NUM_CLASSES)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.L1Loss()\n",
    "# criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "22792",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6fa134d70e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# count batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mlenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mlenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-451b90866fc2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenome_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenome_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mlenv/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mlenv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3103\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3104\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 22792"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "# Training process\n",
    "\n",
    "b = 0 # count batch\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(x_batch) \n",
    "        \n",
    "#         loss = criterion(outputs, y_batch) \n",
    "        loss = criterion(outputs, torch.max(y_batch.type(torch.LongTensor), 1)[1])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (b + 1) % 10 == 0:\n",
    "            print(\"Epoch {}, Batch {}, loss :{}\".format(epoch + 1, b + 1, loss.data[0]))\n",
    "        b = b + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.ByteTensor for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-db7bb8f8bec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hard validation Accuracy: {}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.ByteTensor for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "# Hard Validation process\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for x_batch, y_batch in hardval_loader:\n",
    "    outputs = model(x_batch)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y_batch.size(0)\n",
    "    correct += (predicted == (y_batch[:,1] == 1)).sum()\n",
    "\n",
    "print('Hard validation Accuracy: {}%'.format(100 * correct / total))    \n",
    "\n",
    "\n",
    "# Soft Validation process\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for x_batch, y_batch in softval_loader:\n",
    "    outputs = model(x_batch)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y_batch.size(0)\n",
    "    correct += (predicted == y_batch).sum()\n",
    "\n",
    "print('Soft validation Accuracy: {}%'.format(100 * correct / total))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
